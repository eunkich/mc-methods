{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759dce0b-dee0-4fde-8190-d530caaf885d",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Consider the problem of estimating \n",
    "\n",
    "$$\\theta = \\mathbb{P}[X > a]$$\n",
    "\n",
    "where $X$ is some random variable and $a$ is a constant.  We will assume that $a$ is very large, so that $\\theta$ is very small.  Define \n",
    "\n",
    "$$f_{X}(x; \\beta) = \\frac{e^{\\beta x}f_{X}(x)}{Z(\\beta)}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$Z(\\beta) = \\int_{-\\infty}^{\\infty}e^{\\beta x}f_{X}(x)\\,\\textrm{d}x$$\n",
    "\n",
    "and let $\\mathbb{E}_{\\beta}$ denote expectation with respect to the density $f_{X}(x; \\beta)$.  (This is the same as our notation from class.)  You can assume that the integral in $Z(\\beta)$ converges for any value of $\\beta$.  \n",
    "\n",
    "In class, we showed that \n",
    "\n",
    "$$\\theta = \\mathbb{E}_{\\beta}\\left[1_{(a, \\infty)}(X)e^{-\\beta X + \\ln Z(\\beta)}\\right]$$\n",
    "\n",
    "and we derived a formula for the variance of our importance sampling estimator in the form \n",
    "\n",
    "$$\\textrm{Var}_{\\beta}\\left[1_{(a, \\infty)}(X)e^{-\\beta X + \\ln Z(\\beta)}\\right] = F(\\beta) - \\theta^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$F(\\beta) = \\int_{-\\infty}^{\\infty}1_{(a, \\infty)}(x)e^{-\\beta x + \\ln Z(\\beta)}f_{X}(x)\\,\\textrm{d}x$$\n",
    "\n",
    "(You should confirm to yourself that these match the general formulas we had in class, but there is no need to re-derive them on the homework.)\n",
    "\n",
    "## Part 1\n",
    "\n",
    "In order to minimize the variance of our importance sampling estimator, we need to minimize $F(\\beta)$.  Under some mild assumptions about $X$ and $a$, one can show that there is a unique, optimal value of $\\beta$ and that it satisfies \n",
    "\n",
    "$$F'(\\beta) = 0$$\n",
    "\n",
    "Show that the optimal $\\beta$ satisfies \n",
    "\n",
    "$$\\frac{\\textrm{d}}{\\textrm{d}\\beta}\\ln Z(\\beta) = \\mathbb{E}_{-\\beta}\\left[X\\:\\vert\\: X > a\\right]$$\n",
    "\n",
    "(You can assume that all integrals involved converge uniformly, and so you are free to interchange derivatives and integrals.)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "Under some mild assumptions about $X$, one can show that \n",
    "\n",
    "$$\\lim_{a\\to\\infty}\\mathbb{E}_{-\\beta}\\left[X\\:\\vert\\: X > a\\right] = a$$\n",
    "\n",
    "for $\\beta \\geq 0$.  Combined with part 1, this means that we can approximate the optimal $\\beta$ by solving \n",
    "\n",
    "$$\\frac{\\textrm{d}}{\\textrm{d}\\beta}\\ln Z(\\beta) = a$$\n",
    "\n",
    "Use this formula to approximate the optimal value of $\\beta$ when $X\\sim \\textrm{Exp}(1)$ and $a = 10$.  (We solved this problem in class and found that the true optimal value was $\\beta \\approx 0.90499$.)\n",
    "\n",
    "(In this particular problem, you should find that solving this formula for $\\beta$ is quite simple.  In general, you would typically have to resort to root-finding algorithms like `scipy.optimize.root` to solve this equation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c49a4d-8877-4d1a-b284-773149d8cd2a",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "Suppose $X\\sim\\textrm{Exp}(2)$ and $Y\\sim\\textrm{Exp}(1)$ are independent random variables.  and define $Z = \\max\\{X,\\, Y\\}$. \n",
    "\n",
    "Use exponential tilting on *both* $X$ and $Y$ to estimate \n",
    "\n",
    "$$\\mathbb{P}\\left[Z > 20\\right]$$\n",
    "\n",
    "You do not have to calculate the opitmal value(s) of $\\beta$, but you should give some justification for why your choice is reasonable.  \n",
    "\n",
    "Report your estimate along with a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd83015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0611535811454473e-09"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lamda1, lamda2 = 2, 1\n",
    "def cdf(z):\n",
    "    return (1 - np.exp(-lamda1 * z)) * (1 - np.exp(-lamda2 * z))\n",
    "\n",
    "ans = 1 - cdf(20)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f72781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance Sampling: \n",
      "Theta = 2.0473503210611492e-09\n",
      "Confidence interval: [1.95262720e-09 2.14207344e-09]\n",
      "Exact answer: 2.0611535811454473e-09\n",
      "error: -1.380326008429812e-11\n"
     ]
    }
   ],
   "source": [
    "n = 1000000\n",
    "beta1 = 39/20\n",
    "beta2 = 19/20\n",
    "\n",
    "a = 20\n",
    "zdelta = 1.96\n",
    "\n",
    "# Direct method\n",
    "u1 = np.random.uniform(0, 1, size=n)\n",
    "u2 = np.random.uniform(0, 1, size=n)\n",
    "x = -np.log(u1) / (2-beta1)\n",
    "y = -np.log(u2) / (1-beta2)\n",
    "z = np.maximum(x, y)\n",
    "\n",
    "# Estimate optimal beta\n",
    "print(\"Importance Sampling: \")\n",
    "\n",
    "integrand = (z > a) * 2 * 20 * 20 * np.exp(- beta1 * x - beta2 * y)\n",
    "\n",
    "theta = np.mean(integrand)\n",
    "var = np.var(integrand)\n",
    "error = zdelta * np.sqrt(var / n)\n",
    "ci = np.array([-1, 1]) * error + theta\n",
    "\n",
    "print(f\"Theta = {theta}\")\n",
    "print(f\"Confidence interval: {ci}\")\n",
    "print(f\"Exact answer: {ans}\")\n",
    "print(f\"error: {theta-ans}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ec1dec6-e884-4659-a336-e77dd143620e",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "\n",
    "Suppose $X\\sim N(0, 1)$ and $Y\\sim N(0, 4)$ are independent random variables.  Consider the probability \n",
    "\n",
    "$$\\theta = \\mathbb{P}\\left[V > 10\\right]$$\n",
    "\n",
    "where $V = X + Y$.  \n",
    "\n",
    "## Part 1\n",
    "\n",
    "Estimate $\\theta$ using direct Monte Carlo simulation.  Make sure you use a large enough sample size to get a valid estimate.  Report your estimate along with a 95% confidence interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35f1c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: 4.12e-06\n",
      "var: 4.1199830256007065e-06\n",
      "ci: [3.72216427e-06 4.51783573e-06]\n"
     ]
    }
   ],
   "source": [
    "n = 100000000\n",
    "z1 = np.random.randn(n)\n",
    "z2 = np.random.randn(n)\n",
    "x = z1\n",
    "y = z2 * 2\n",
    "v = x + y\n",
    "hv = (x + y) > 10\n",
    "\n",
    "theta = np.mean(hv)\n",
    "var = np.var(hv)\n",
    "error = zdelta * np.sqrt(var/n)\n",
    "ci = error * np.array([-1, 1]) + theta\n",
    "\n",
    "print(f\"theta: {theta}\")\n",
    "print(f\"var: {var}\")\n",
    "print(f\"ci: {ci}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecd25a9",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2\n",
    "\n",
    "Estimate $\\theta$ using exponential tilting in $V$.  You can use the fact that the moment generating function of a normal random variable with mean $\\mu$ and variance $\\sigma^2$ is given by \n",
    "\n",
    "$$Z(\\beta) = e^{\\beta\\mu + \\sigma^2\\beta^2/2}$$\n",
    "\n",
    "You should approximate the optimal value of $\\beta$ using the technique outlined in problem 1.  Report your estimate along with a 95% confidence interval. How much did this reduce the error from part 1? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "db0cf7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: 3.872545621313534e-06\n",
      "var: 7.584969289236722e-11\n",
      "ci: [3.87083862e-06 3.87425262e-06]\n",
      "reduction: 99.57092896294353%\n"
     ]
    }
   ],
   "source": [
    "beta = 2\n",
    "N = 100000000\n",
    "z = np.random.randn(N)\n",
    "v = np.sqrt(5) * z + 5 * beta\n",
    "\n",
    "def Z(beta):\n",
    "    return np.exp((5 * beta ** 2) / 2)\n",
    "\n",
    "integrand = (v > 10) * np.exp(-beta * v) * Z(2)\n",
    "theta = np.mean(integrand)\n",
    "var = np.var(integrand)\n",
    "error_ = zdelta * np.sqrt(var/n)\n",
    "ci = error_ * np.array([-1, 1]) + theta\n",
    "reduction = 100 * (1 - error_ / error)\n",
    "\n",
    "print(f\"theta: {theta}\")\n",
    "print(f\"var: {var}\")\n",
    "print(f\"ci: {ci}\")\n",
    "print(f\"reduction: {reduction}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50015790",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3\n",
    "\n",
    "Repeat part 2, but this time approximate the optimal value of $\\beta$ using the method described in lecture 15.  (I.e., approximate $F(\\beta)$ and then minimize that function numerically.)  Report your estimate along with a 95% confidence interval. How much did this reduce the error from part 1? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c3c9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "N = 1000000\n",
    "z = np.random.randn(N)\n",
    "v = np.sqrt(5) * z\n",
    "\n",
    "def Z(beta):\n",
    "    return np.exp((5 * beta ** 2) / 2)\n",
    "\n",
    "def F_hat(beta, v):\n",
    "    out = (v > 10) ** 2 * np.exp(-beta * v) * Z(beta)\n",
    "    return out.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "50cf84eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: Optimization terminated successfully.\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 1.6138110908526286e-10\n",
       "        x: [ 2.015e+00]\n",
       "      nit: 21\n",
       "      jac: [-4.720e-11]\n",
       " hess_inv: [[ 7.704e+08]]\n",
       "     nfev: 52\n",
       "     njev: 26"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = minimize(lambda beta: F_hat(beta, v), 0, tol=1e-10)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "09310a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0154628390296314"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat = out.x[0]\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e0f876c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: 3.3151853473187206e-06\n",
      "var: 5.5383838232691075e-11\n",
      "ci: [3.31372671e-06 3.31664399e-06]\n",
      "reduction: 99.63335665058867%\n"
     ]
    }
   ],
   "source": [
    "beta = beta_hat\n",
    "N = 100000000\n",
    "z = np.random.randn(N)\n",
    "v = np.sqrt(5) * z + 5 * beta\n",
    "\n",
    "integrand = (v > 10) * np.exp(-beta * v) * Z(2)\n",
    "theta = np.mean(integrand)\n",
    "var = np.var(integrand)\n",
    "error_ = zdelta * np.sqrt(var/n)\n",
    "ci = error_ * np.array([-1, 1]) + theta\n",
    "reduction = 100 * (1 - error_ / error)\n",
    "\n",
    "print(f\"theta: {theta}\")\n",
    "print(f\"var: {var}\")\n",
    "print(f\"ci: {ci}\")\n",
    "print(f\"reduction: {reduction}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4ace4fe-541a-4a95-aa9c-b99ab26e6fe5",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Consider the random variable \n",
    "\n",
    "$$L = \\sum_{i = 1}^{m}X_i$$\n",
    "\n",
    "where $X_i = c_i Y_i$ and the $c_i$ are positive constants and $Y_i\\sim\\textrm{Bern}(p_i)$ are independent (but not necessarily identically distributed).  We want to estimate \n",
    "\n",
    "$$\\theta = \\mathbb{P}[L > \\ell]$$\n",
    "\n",
    "We already did this in class when the $c_i$ and $p_i$ were all the same, but now we will look at a case where they vary with $i$.  In particular, suppose that $m = 10$ and \n",
    "\n",
    "$$c_i = i + 1 \\:\\textrm{ and }\\: p_i = \\frac{1}{i + 1}$$\n",
    "\n",
    "for $i = 1, \\dotsc, 10$ and $\\ell = 40$.\n",
    "\n",
    "In each part, you should implement your method with at least $100,000$ samples.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6edff895",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1\n",
    "\n",
    "Estimate $\\theta$ using direct Monte Carlo simulation.  Report your estimate along with a 95% confidence interval.  (You can use the code from class as a template, so this should be easy.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "533bbf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta: 0.000737\n",
      "var: 0.0007364568310000001\n",
      "ci: [0.00073168 0.00074232]\n"
     ]
    }
   ],
   "source": [
    "c = np.arange(1, 11) + 1\n",
    "p = 1 / c\n",
    "\n",
    "m = 10\n",
    "l = 40\n",
    "\n",
    "N = 1000000\n",
    "\n",
    "loss = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    u = np.random.uniform(0, 1, m)\n",
    "    loss[i] = (c @ (u < p)) > l\n",
    "\n",
    "theta = np.mean(loss)\n",
    "var = np.var(loss)\n",
    "error = zdelta * np.sqrt(var/n)\n",
    "ci = error * np.array([-1, 1]) + theta\n",
    "\n",
    "print(f\"theta: {theta}\")\n",
    "print(f\"var: {var}\")\n",
    "print(f\"ci: {ci}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa567c5",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2\n",
    "\n",
    "Estimate $\\theta$ using exponential tilting in each of the $X_i$'s.  Use the same value of $\\beta$ for each $X_i$.  You should approximate the optimal value of $\\beta$ using either the approach from problem 1 or from lecture 15.  (You can use the code from class as a template.  The hard part of this problem is finding the optimal $\\beta$.)\n",
    "\n",
    "**Hint**: The moment generating function of a sum of random variables is the product of their moment generating functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909851e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
